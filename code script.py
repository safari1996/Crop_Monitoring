# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NO3XOInvrcpvVu8FAnsaG7uoxnhJ3Er2
"""

# ==========================================
# 🌽 Crop Yield Analysis with Remote Sensing
# IEEE J-STARS Special Issue: Data Quality
# ==========================================

# 📦 1. Install Required Libraries (if needed)
# Uncomment below lines if libraries are not preinstalled
!pip install pandas matplotlib seaborn scikit-learn

# 📥 2. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

from google.colab import files
uploaded = files.upload()

# 🧼 4. Read and Clean the Dataset
df_raw = pd.read_csv(next(iter(uploaded)))
df = df_raw[1:].copy()  # Remove the first row (repeated headers)
df.columns = df_raw.iloc[0]  # Set proper headers
df.reset_index(drop=True, inplace=True)

# 🧹 5. Convert Numeric Columns
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors='ignore')

# 🔍 6. Basic Data Overview
print("Shape of dataset:", df.shape)
print("Columns:\n", df.columns.tolist())
df.describe(include='all').T

# ⚠️ 7. Data Quality Assessment
missing = df.isnull().sum()
missing_percent = (missing / len(df)) * 100
missing_summary = pd.DataFrame({'Missing Count': missing, 'Missing %': missing_percent})
print("Missing Values Summary:\n", missing_summary[missing_summary['Missing Count'] > 0])

# 📊 8. Visualize Missing Data
plt.figure(figsize=(12,6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Data Heatmap")
plt.show()

# 🧪 9. Fix GPP Scaling Issue
gpp_columns = [col for col in df.columns if 'GPP' in str(col)]
df[gpp_columns] = df[gpp_columns].astype(float) / 10000

# 📈 10. Correlation Heatmap
numeric_cols = df.select_dtypes(include=['float64', 'int']).columns
plt.figure(figsize=(18, 12))
sns.heatmap(df[numeric_cols].corr(), cmap='coolwarm', annot=False)
plt.title("Correlation Heatmap")
plt.show()

# 🎯 11. Yield Prediction Modeling
target = 'yield'
features = gpp_columns + [
    'NDVI_1', 'NDVI_2', 'NDVI_3', 'EVI_1', 'EVI_2', 'EVI_3',
    'PPT_1', 'TMEAN_1', 'VPDMIN_1', 'cec', 'clay_percent', 'organic_matter'
]

# Remove rows with missing values in selected columns
df_model = df[features + [target]].dropna()

X = df_model[features]
y = df_model[target].astype(float)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"📉 RMSE: {rmse:.2f}")
print(f"📊 R² Score: {r2:.2f}")

# 📌 12. Feature Importance
importances = pd.Series(model.feature_importances_, index=features)
importances.sort_values(ascending=True).plot(kind='barh', figsize=(10, 8))
plt.title("Feature Importance")
plt.xlabel("Relative Importance")
plt.show()

# Simulate missing data in NDVI to test model robustness
df_quality_test = df_model.copy()
np.random.seed(42)
missing_mask = np.random.rand(*df_quality_test['NDVI_1'].shape) < 0.2
df_quality_test.loc[missing_mask, 'NDVI_1'] = np.nan

# Drop NA and retrain
df_qtest = df_quality_test.dropna()
X_qtest = df_qtest[features]
y_qtest = df_qtest[target].astype(float)

X_train_q, X_test_q, y_train_q, y_test_q = train_test_split(X_qtest, y_qtest, test_size=0.2, random_state=42)

model_q = RandomForestRegressor(n_estimators=100, random_state=42)
model_q.fit(X_train_q, y_train_q)
y_pred_q = model_q.predict(X_test_q)

rmse_q = np.sqrt(mean_squared_error(y_test_q, y_pred_q))
r2_q = r2_score(y_test_q, y_pred_q)

print(f"With simulated missing NDVI:")
print(f"📉 RMSE: {rmse_q:.2f}")
print(f"📊 R² Score: {r2_q:.2f}")

plt.figure(figsize=(10, 6))
sns.boxplot(data=[y_test.values, y_pred], orient='h')
plt.yticks([0, 1], ['Actual Yield', 'Predicted Yield'])
plt.title("Boxplot Comparison of Actual vs Predicted Yield")
plt.show()

residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
sns.histplot(residuals, bins=30, kde=True)
plt.title("Residuals Distribution (Actual - Predicted Yield)")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.axvline(0, color='red', linestyle='--')
plt.show()

results_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': y_pred,
    'Residual': residuals
})
results_df.to_csv("yield_prediction_results.csv", index=False)
print("📁 Results saved for use in paper figures.")

# 📊 Metrics Table
metrics_table = pd.DataFrame({
    'Metric': ['RMSE', 'R² Score'],
    'Value': [rmse, r2]
})
print("\n📈 Model Performance Summary:")
print(metrics_table.to_string(index=False))

#19. Figure: Actual vs Predicted Scatter Plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, s=60)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("Actual Yield")
plt.ylabel("Predicted Yield")
plt.title("Scatter Plot: Actual vs Predicted Yield")
plt.grid(True)
plt.tight_layout()
plt.savefig("scatter_actual_vs_predicted.png", dpi=300)
plt.show()

#20. Figure: Residual Histogram
residuals = y_test - y_pred
plt.figure(figsize=(8, 5))
sns.histplot(residuals, bins=30, kde=True, color='coral')
plt.axvline(0, color='black', linestyle='--')
plt.title("Histogram of Residuals (Prediction Error)")
plt.xlabel("Residual (Actual - Predicted)")
plt.ylabel("Frequency")
plt.tight_layout()
plt.savefig("residuals_histogram.png", dpi=300)
plt.show()

#22. Figure: Feature Importance (Already Done in Section 12)
plt.figure(figsize=(10, 7))
importances.sort_values(ascending=True).plot(kind='barh', color='forestgreen')
plt.title("Feature Importance (Random Forest)")
plt.xlabel("Relative Importance")
plt.tight_layout()
plt.savefig("feature_importance.png", dpi=300)
plt.show()

#23. Table: Feature Correlation with Yield
correlations = df[features + ['yield']].corr()['yield'].drop('yield').sort_values(ascending=False)
correlation_table = correlations.reset_index()
correlation_table.columns = ['Feature', 'Correlation with Yield']
print("📊 Feature Correlation Table:")
print(correlation_table)
correlation_table.to_csv("feature_correlation_table.csv", index=False)

#25. Compare Multiple Models: XGBoost vs Random Forest
from xgboost import XGBRegressor

# Same train-test split as before
xgb_model = XGBRegressor(n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

# Evaluation
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2_xgb = r2_score(y_test, y_pred_xgb)

print("\n📊 Model Comparison:")
print(f"Random Forest - RMSE: {rmse:.2f}, R²: {r2:.2f}")
print(f"XGBoost       - RMSE: {rmse_xgb:.2f}, R²: {r2_xgb:.2f}")

# Plot comparison
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred, label='Random Forest', alpha=0.6)
sns.scatterplot(x=y_test, y=y_pred_xgb, label='XGBoost', alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.xlabel("Actual Yield")
plt.ylabel("Predicted Yield")
plt.title("Model Comparison: RF vs XGBoost")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("model_comparison_rf_xgb.png", dpi=300)
plt.show()

#26. Inject Missing Values to Test Model Robustness
df_corrupt = df_model.copy()
np.random.seed(42)

# Introduce missing values into NDVI_1 (~20% missing)
mask_missing = np.random.rand(len(df_corrupt)) < 0.2
df_corrupt.loc[mask_missing, 'NDVI_1'] = np.nan

df_corrupt_clean = df_corrupt.dropna()
X_c = df_corrupt_clean[features]
y_c = df_corrupt_clean[target]

X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_c, y_c, test_size=0.2, random_state=42)
model_c = RandomForestRegressor(n_estimators=100, random_state=42)
model_c.fit(X_train_c, y_train_c)
y_pred_c = model_c.predict(X_test_c)

rmse_c = np.sqrt(mean_squared_error(y_test_c, y_pred_c))
r2_c = r2_score(y_test_c, y_pred_c)

print(f"\n🧪 Impact of Missing NDVI_1:")
print(f"RMSE (with missing): {rmse_c:.2f}")
print(f"R² (with missing): {r2_c:.2f}")

#27. Simulate Sensor Alignment Error (Column Shift)
df_misaligned = df_model.copy()

# Shift GPP values by 1 time step
for i in range(1, len(gpp_columns)):
    df_misaligned[gpp_columns[i]] = df_model[gpp_columns[i-1]]

# Train and evaluate with misaligned GPP
X_mis = df_misaligned[features]
y_mis = df_misaligned[target]

X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_mis, y_mis, test_size=0.2, random_state=42)
model_m = RandomForestRegressor(n_estimators=100, random_state=42)
model_m.fit(X_train_m, y_train_m)
y_pred_m = model_m.predict(X_test_m)

rmse_m = np.sqrt(mean_squared_error(y_test_m, y_pred_m))
r2_m = r2_score(y_test_m, y_pred_m)

print(f"\n🚨 Impact of Simulated Sensor Misalignment (GPP shift):")
print(f"RMSE (misaligned): {rmse_m:.2f}")
print(f"R² (misaligned): {r2_m:.2f}")